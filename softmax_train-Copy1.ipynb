{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model \n",
    "from keras.layers import Flatten, Dropout, Dense, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from utility import preprocess_image\n",
    "from keras.models import load_model \n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, 'model')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Training Config Parameters\n",
    "run_dir = 'train_run'\n",
    "nb_train_samples = 18190\n",
    "nb_validation_samples = 2022\n",
    "save_step = 5\n",
    "epochs = 30000\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "momentum = 0.99\n",
    "fine_tuning_ture = 0\n",
    "fine_tuning_model_path = \"./train_run/XXX.hdf5\"\n",
    "initial_epoch=0\n",
    "\n",
    "# GPU Usage Control\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare network\n",
    "# base model selection\n",
    "bm = 2\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 100,100\n",
    "num_classes = 452\n",
    "#0 -- VGG16 (224x224)\n",
    "#1 -- Mobilenet (224x224)\n",
    "#2 -- CASIA (100*100)\n",
    "\n",
    "#Strarting from random initialization\n",
    "#if fine tuning from pre-trained model : weight = 'imagenet'\n",
    "if(fine_tuning_ture == 0):\n",
    "    #define input image size\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "    #load predefined base network model\n",
    "    if (bm==0):\n",
    "        from keras.applications.vgg16 import VGG16 #VGG\n",
    "        base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif (bm==1):\n",
    "        from keras.applications.mobilenet import MobileNet #Mobilenet\n",
    "        base_model = MobileNet(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif (bm==2):\n",
    "        from casia import CASIA \n",
    "        base_model = CASIA(input_shape=input_shape)\n",
    "    \n",
    "    x = base_model.output\n",
    "    \n",
    "    if bm == 2:\n",
    "        x = Flatten()(x)\n",
    "    \n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "else:\n",
    "    model = load_model(fine_tuning_model_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 32)      320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 96)        110688    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 192)       166080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 128)       221312    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 160)         368800    \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 320)         461120    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 320)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 452)               145092    \n",
      "=================================================================\n",
      "Total params: 1,897,860\n",
      "Trainable params: 1,897,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#para has problem\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18109 images belonging to 452 classes.\n",
      "Found 2103 images belonging to 452 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare ImageDataGenerator\n",
    "train_data_dir = './dataset/lfw_mtcnnpy_160_gray_train_valid/train'\n",
    "validation_data_dir = './dataset/lfw_mtcnnpy_160_gray_train_valid/valid'\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.9,\n",
    "#     height_shift_range=0.9,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_image)\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    color_mode=\"grayscale\",\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000\n",
      "2273/2273 [==============================] - 203s 89ms/step - loss: 15.7161 - acc: 0.0029 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 2/30000\n",
      "2273/2273 [==============================] - 247s 109ms/step - loss: 16.0782 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 3/30000\n",
      "2273/2273 [==============================] - 406s 179ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 4/30000\n",
      "2273/2273 [==============================] - 350s 154ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 5/30000\n",
      "2273/2273 [==============================] - 358s 158ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 6/30000\n",
      "2273/2273 [==============================] - 329s 145ms/step - loss: 16.0755 - acc: 0.0026 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 7/30000\n",
      "2273/2273 [==============================] - 256s 112ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 8/30000\n",
      "2273/2273 [==============================] - 281s 124ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025ss: 16.0789 - acc: 0.002 - ETA: 30s  - ETA: 27s - loss: 16.0785 - acc: 0.0 - ETA: 27s - loss: 16.0785 - a - ETA: 15s - los - ETA: 12s - loss: 16 - ETA: 5s - loss: 16.0792 - acc - ETA: 4s - loss:\n",
      "Epoch 9/30000\n",
      "2273/2273 [==============================] - 262s 115ms/step - loss: 16.0755 - acc: 0.0026 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 10/30000\n",
      "2273/2273 [==============================] - 69s 30ms/step - loss: 16.0809 - acc: 0.0023 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 11/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 12/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 13/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0800 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 14/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 15/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 16/30000\n",
      "2273/2273 [==============================] - 30s 13ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 17/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 18/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0768 - acc: 0.0026 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 19/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0800 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 20/30000\n",
      "2273/2273 [==============================] - 30s 13ms/step - loss: 16.0755 - acc: 0.0026 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 21/30000\n",
      "2273/2273 [==============================] - 31s 13ms/step - loss: 16.0773 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 22/30000\n",
      "2273/2273 [==============================] - 30s 13ms/step - loss: 16.0818 - acc: 0.0023 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 23/30000\n",
      "2273/2273 [==============================] - 31s 14ms/step - loss: 16.0782 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 24/30000\n",
      "2273/2273 [==============================] - 30s 13ms/step - loss: 16.0782 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 25/30000\n",
      "2273/2273 [==============================] - 30s 13ms/step - loss: 16.0755 - acc: 0.0026 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 26/30000\n",
      "2273/2273 [==============================] - 29s 13ms/step - loss: 16.0800 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 27/30000\n",
      "2273/2273 [==============================] - 38s 17ms/step - loss: 16.0750 - acc: 0.0027 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 28/30000\n",
      "2273/2273 [==============================] - 44s 19ms/step - loss: 16.0782 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 29/30000\n",
      "2273/2273 [==============================] - 47s 21ms/step - loss: 16.0782 - acc: 0.0025 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 30/30000\n",
      "2273/2273 [==============================] - 50s 22ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 31/30000\n",
      "2273/2273 [==============================] - 104s 46ms/step - loss: 16.0791 - acc: 0.0024 - val_loss: 16.0781 - val_acc: 0.0025\n",
      "Epoch 32/30000\n",
      "1595/2273 [====================>.........] - ETA: 1:05 - loss: 16.0764 - acc: 0.0026"
     ]
    }
   ],
   "source": [
    "#start to train\n",
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=learning_rate, momentum=momentum), metrics=['accuracy']    )\n",
    "\n",
    "#setup check point\n",
    "filepath=os.path.join(run_dir, \"weights-improvement-{epoch:02d}-{val_loss:.2f}-{acc:.2f}-{val_acc:.2f}.hdf5\")\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath,period=save_step)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#start to train\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
